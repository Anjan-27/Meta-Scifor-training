{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8274c3b3-4e02-4a25-8b34-44beb18ef1b9",
   "metadata": {},
   "source": [
    "## CNN Classification and Object Detection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eebf6-8ca3-4af7-a085-1649c109cf0c",
   "metadata": {},
   "source": [
    "#### 1. How does the architecture of a CNN designed for image classification differ from one used for object detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1552a55-a3e4-4386-bfb9-01015ce34dc6",
   "metadata": {},
   "source": [
    "Ans:  In classification if we are given an image we can predict what kind of object is present in image , but does not get where exactly the image of object is. So in that case we use object detection . Basically image classification outputs a single class predictions whereas object detection identify and locate multiple objects in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a400456-f5cb-417c-9d22-271bf4070752",
   "metadata": {},
   "source": [
    "#### 2. What is the role of a Region Proposal Network (RPN) in object detection models like Faster R-CNN, and how does it help in identifying objects in an image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14b5d9-a506-47ed-8400-a5ba5b78f927",
   "metadata": {},
   "source": [
    "Ans: In RPN we do not have input image basically the input is feature map. In models like fast RCNN , the network uses RPN to replace selective search.\n",
    "The RPN operates by sliding a small network over the convolutional feature map of the image. At each sliding window position, the RPN generates multiple anchor boxesand predicts whether each box contains an object or not. The RPN also refines the anchor box locations based on the image content. The RPN narrows down the search space by generating region proposals, allowing the network to focus on areas likely to contain objects. This speeds up the object detection process while improving accuracy, as the network does not need to evaluate the entire image exhaustively.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5818ce9-b1f4-4c85-aaea-28932d9e5306",
   "metadata": {},
   "source": [
    "####  3. Explain how transfer learning can be applied to a CNN for both image classification and object detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1a077-b77c-4130-8145-4ae33c782ddd",
   "metadata": {},
   "source": [
    "Ans:Transfer learning can be applied to a Convolutional Neural Network (CNN) for image classification and object detection by using a pre-trained model as a starting point for a new task  \n",
    "Image Classification : The pretrained layers are fine tunned to suit the target.\n",
    "\n",
    "Object Detection :Use a pre-trained modelâ€™s backbone (like ResNet) for feature extraction, then attach an object detection head (like RPN or SSD) for bounding box and class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c77d23-a479-4669-a116-e086296d1b92",
   "metadata": {},
   "source": [
    "#### 4. What is the significance of anchor boxes in object detection models, and how do they assist CNNs in predicting object locations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32242896-b309-4954-8409-06a3ce55c95b",
   "metadata": {},
   "source": [
    "Ans: Anchors- You have an image and for every pixel in the image you have a center point known as Anchors. Anchor Boxes are predefined boxes of different sizes and aspect ratios used to predict objects in various scales and shapes. Each anchor box is adjusted to better fit the object, helping the objet to handle multiple objects at different locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1103950-d196-4cd3-a9bc-79159781315a",
   "metadata": {},
   "source": [
    "#### 5. Compare the loss functions used in CNN-based image classification (e.g., cross-entropy loss) and object detection (e.g., localization loss and classification loss). How are they combined in object detection tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94a548-e810-49f0-8b30-bc74cf7f08e7",
   "metadata": {},
   "source": [
    "Ans: In Image Classification we use cross-entropy loss to measure classification accuracy.\n",
    "In Object Detection we uses both classification loss (cross-entropy) and localization loss (smooth L1 loss) to measure bounding box accuracy. In object detection, these two losses are combined (often added) to train the model to predict both the object class and its location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21f7a3-82fc-493c-9390-a9da73820c35",
   "metadata": {},
   "source": [
    "#### 6. How does the role of fully connected layers in CNNs for image classification differ from their role (or absence) in object detection networks like YOLO and SSD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f889af-3a0d-4f5b-b02b-4d86e88ba703",
   "metadata": {},
   "source": [
    "Ans: In Image Classification , fully connected layers are used to convert extracted feature maps at the end to output class probabilities.\n",
    "In Object Detection (YOLO/SSD),Fully connected layers are often removed. Instead, these models use convolutional layers to directly predict bounding boxes and class probabilities across the entire image grid.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4112b5-0561-4290-a22c-0cf41ceec6fa",
   "metadata": {},
   "source": [
    "#### 7. What are the key architectural characteristics of the VGG network, and how does its deep, sequential structure contribute to improved performance in image classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bda796-06f7-4441-a59e-b9dcda7071ed",
   "metadata": {},
   "source": [
    "Ans: VGG is characterized by deep, sequential layers with small convolutional 3x3 filters and max-pooling layers to increase network capacity. The depth enables the network to learn hierarchical features, which improves performance on image classification tasks by capturing more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a94ab8-14ea-4bf6-a40a-cf23e67e26be",
   "metadata": {},
   "source": [
    "#### 8. Explain how Non-Maximum Suppression (NMS) is used in object detection models to eliminate redundant bounding boxes and improve detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfe2ea-6575-48f9-b3de-99341353df83",
   "metadata": {},
   "source": [
    "Ans: Basically if there are lots of anchor boxes and they have a very big overlap with each other, but all of them have have IOU > requirement IOU. S in that case we use Non-Maximum Suppression. \n",
    "It only uses those anchor boxes that have highest overlap.\n",
    "Loss = Classifier(log loss) + Regression (Smooth L1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267900f-0029-4ee8-87ce-81c0db3e5b11",
   "metadata": {},
   "source": [
    "#### 9. In a CNN-based object detection model like YOLO, how is the concept of grid cells used to predict multiple bounding boxes in an image, and how does it affect the model's efficiency and accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babf99e-fc2e-4239-a817-1e7ab8b06d0a",
   "metadata": {},
   "source": [
    "Ans: The goal of YOLO is to identify and classify objects in single pass of image .In YOLO the image is divided into a grid, and each cell is responsible for predicting bounding boxes and object classes within that cell. This approach allows YOLO to predict multiple objects at different locations simultaneously, improving efficiency and speed, though it may reduce accuracy for small objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121bba90-2648-4437-884d-b228a485d788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
